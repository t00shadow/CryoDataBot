{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "# from B1_UTILS import download_map_model, map_normalizing, map_output\n",
    "# from B2_UTILS_data2npy import data_to_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = True\n",
    "NORMALIZATION = True\n",
    "MAP2NPY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARTS = ['secondary_strctures', 'key_atoms', 'residue_types']\n",
    "\n",
    "MAIN_HOME_PATH = '/home/qiboxu/Database/U_NET/EMDB_PDB_for_U_Net'\n",
    "PATH_SETTINGS = {\n",
    "    \"Filtered_Dateset\": [\n",
    "        os.path.join(MAIN_HOME_PATH, 'Filtered_Dateset', 'Raw'),\n",
    "        os.path.join(MAIN_HOME_PATH, 'Filtered_Dateset', 'Training'),\n",
    "        os.path.join(MAIN_HOME_PATH, 'Filtered_Dateset', 'Raw', 'final-20240212.csv'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "PATH_KEYS = \"Filtered_Dateset\"\n",
    "DATA_PATH, HOME_PATH, csv_path = PATH_SETTINGS[PATH_KEYS]\n",
    "temp_sample_path = os.path.join(HOME_PATH, \"ready_to_train_and_val\")\n",
    "os.makedirs(temp_sample_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read map list and generate raw_map and model downloading paths\n",
    "df = pd.read_csv(csv_path)\n",
    "emdbs, pdbs = df[\"emdb_id\"], df[\"fitted_pdbs\"]\n",
    "resolutions = df[\"resolution\"].astype(str)\n",
    "emdb_ids = [emdb.split(\"-\")[1] for emdb in emdbs]\n",
    "folders = [f\"{emdb}_re_{resolution}\" for emdb, resolution in zip(emdbs, resolutions)]\n",
    "raw_maps = [f\"emd_{emdb_id}.map\" for emdb_id in emdb_ids]\n",
    "models = [f\"{pdb}.cif\" for pdb in pdbs]\n",
    "raw_map_paths = [f\"{DATA_PATH}/{folder}/{raw_map}\" for folder, raw_map in zip(folders, raw_maps)]\n",
    "model_paths = [f\"{DATA_PATH}/{folder}/{model}\" for folder, model in zip(folders, models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_paths = [f\"{raw_map_path.split('.map')[0]}_normalized.mrc\" for raw_map_path in raw_map_paths]\n",
    "for idx, emdb_id in enumerate(emdb_ids):\n",
    "    if DOWNLOAD:\n",
    "        download_map_model(emdb=emdbs[idx], pdb=pdbs[idx], resolution=resolutions[idx], directory=DATA_PATH)\n",
    "        # if idx == 20:   # for download limited number of testing data\n",
    "        #     exit()\n",
    "    if NORMALIZATION and not os.path.exists(map_paths[idx]):\n",
    "        map_data = map_normalizing(raw_map_paths[idx])\n",
    "        map_output(raw_map_paths[idx], map_data, map_paths[idx], is_model = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = False\n",
    "NORMALIZATION = False\n",
    "MAP2NPY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset from EMDB-11893... \n",
      "Sampling part: secondary_strctures\n",
      "Sampling part: key_atoms\n",
      "Sampling part: residue_types\n",
      "Generating dataset from EMDB-22345... \n",
      "Sampling part: secondary_strctures\n",
      "Sampling part: key_atoms\n",
      "Sampling part: residue_types\n",
      "Generating dataset from EMDB-12884... \n",
      "Sampling part: secondary_strctures\n",
      "Sampling part: key_atoms\n",
      "Sampling part: residue_types\n",
      "Generating dataset from EMDB-10892... \n",
      "Sampling part: secondary_strctures\n",
      "Sampling part: key_atoms\n",
      "Sampling part: residue_types\n",
      "Generating dataset from EMDB-10914... \n",
      "Sampling part: secondary_strctures\n",
      "Sampling part: key_atoms\n",
      "Sampling part: residue_types\n"
     ]
    }
   ],
   "source": [
    "if MAP2NPY:\n",
    "    # Create training and testing dataset (3d numpy arraies) from map file and model file\n",
    "    # idx of npy file\n",
    "    sample_num = 0\n",
    "\n",
    "    # num of tag in each part for all models\n",
    "    num_of_tag_in_each_part_for_all_models = {}\n",
    "    for part in MODEL_PARTS:\n",
    "        num_of_tag_in_each_part_for_all_models[part] = 0\n",
    "\n",
    "    for idx, emdb_id in enumerate(emdb_ids[0:5]):\n",
    "        # for idx in [0, 1]:\n",
    "        print(f\"Generating dataset from EMDB-{emdb_id}... \")\n",
    "        sample_num, num_of_tag_in_each_part = data_to_npy(\n",
    "            map_paths[idx], model_paths[idx], MODEL_PARTS, temp_sample_path,\n",
    "            sample_num)\n",
    "        # Add number of each tag in new sampled model\n",
    "        for key, value_list in num_of_tag_in_each_part.items():\n",
    "            num_of_tag_in_each_part_for_all_models[key] += value_list\n",
    "        # print(num_of_tag_in_each_part_for_all_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of .npy file: 710\n",
      "Num of each tag: \n",
      "secondary_strctures: [174890136   1590260    706409   2079118   6856317]\n",
      "key_atoms: [180078192   2201966    850162   2991920]\n",
      "residue_types: [180928354    181975    184653     88626     96369     25327    137842\n",
      "     83586    148467     53821    132074    182337    183722     51573\n",
      "     72989     80491    120068    119334     17296     60128    181288\n",
      "    814910    864319    644418    668273]\n",
      "\n",
      "Ratio of tags: \n",
      "secondary_strctures: [1, 109, 247, 84, 25]\n",
      "key_atoms: [1, 81, 211, 60]\n",
      "residue_types: [1, 994, 979, 2041, 1877, 7143, 1312, 2164, 1218, 3361, 1369, 992, 984, 3508, 2478, 2247, 1506, 1516, 10460, 3009, 998, 222, 209, 280, 270]\n"
     ]
    }
   ],
   "source": [
    "# Delete 0s in tag number list and calculate 1/ratio of tag number\n",
    "ratio_of_tag = {}\n",
    "for key, value_list in num_of_tag_in_each_part_for_all_models.items():\n",
    "    value_list = np.trim_zeros(value_list)\n",
    "    num_of_tag_in_each_part_for_all_models[key] = value_list\n",
    "    if 0 in value_list:\n",
    "        print(f'There is missing groups in part {key}, numbers of each class are {value_list}.')\n",
    "    ratio_of_tag[key] = [int(value_list[0]//value_x)\n",
    "                         for value_x in value_list if value_x != 0]\n",
    "\n",
    "# Print statistical results\n",
    "print(f\"The number of .npy file: {sample_num}\")\n",
    "print(\"Num of each tag: \")\n",
    "for key, value in num_of_tag_in_each_part_for_all_models.items():\n",
    "    print(f'{key}: {value}')\n",
    "print(\"\\nRatio of tags: \")\n",
    "for key, value in ratio_of_tag.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2840 files [00:00, 60501.43 files/s]\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing dataset\n",
    "import splitfolders\n",
    "sample_path = os.path.join(HOME_PATH, \"train_val_data\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "splitfolders.ratio(input=temp_sample_path, output=sample_path,\n",
    "                    seed=44, ratio=(.8, .2), group_prefix=None, move=True)\n",
    "shutil.rmtree(temp_sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print weight (ratio of tags)\n",
    "os.makedirs(os.path.join(HOME_PATH, \"TEST\"), exist_ok=True)\n",
    "with open(f\"{HOME_PATH}/TEST/train_val_data/class_weight_for_training.txt\", \"w\") as file:\n",
    "    json.dump(ratio_of_tag, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generatred data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for num in range(0, 5):\n",
    "    model_type = 'map_sample'\n",
    "    mapname = f'/home/qiboxu/Database/U_NET/EMDB_PDB_for_U_Net/Filtered_Dateset/Training/TEST/train_val_data/train/{model_type}/map.{num}.npy'\n",
    "    aaa = np.load(mapname)\n",
    "    # print(np.max(aaa))\n",
    "    plt.imshow(aaa[:, :, 30], cmap='gray', origin='lower')\n",
    "    plt.title(f'{model_type}.{num}')\n",
    "    plt.show()\n",
    "    for model_type in ['secondary_strctures', 'key_atoms', 'residue_types']:\n",
    "        filename = f'/home/qiboxu/Database/U_NET/EMDB_PDB_for_U_Net/Filtered_Dateset/Training/TEST/train_val_data/train/{model_type}/model_{model_type}.{num}.npy'\n",
    "        aaa = np.load(filename)\n",
    "        plt.imshow(aaa[:, :, 30], origin='lower')\n",
    "        plt.title(f'{model_type}.{num}')\n",
    "        plt.show()\n",
    "    print(\"###\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0.1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
