{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import gemmi\n",
    "import matplotlib.pyplot as plt\n",
    "import mrcfile\n",
    "import numpy as np\n",
    "\n",
    "from MRC import MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMDid, resol, PDBid = '3551', '3.25', '5mrc'\n",
    "EMDid, resol, PDBid = '10973', '2.74', '6yws'\n",
    "\n",
    "\n",
    "MAIN_PATH = '/home/qiboxu/Database/U_NET/EMDB_PDB_for_U_Net/Filtered_Dateset/Raw/'\n",
    "map_path = MAIN_PATH + f'EMD-{EMDid}_re_{resol}/emd_{EMDid}_normalized.mrc'\n",
    "model_path = MAIN_PATH + f'EMD-{EMDid}_re_{resol}/{PDBid}.cif'\n",
    "\n",
    "model_parts = ['secondary_strctures', 'key_atoms', 'residue_types']\n",
    "sample_dir = '/home/qiboxu/Database/U_NET/EMDB_PDB_for_U_Net/Filtered_Dateset/Training/ready_to_train_and_val'\n",
    "npy_size = 64\n",
    "data = []\n",
    "part_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mrcfile.mmap(map_path) as mrc:\n",
    "#     map_data = mrc.data\n",
    "#     # head_nz, head_ny, head_nx = mrc.header.nz, mrc.header.ny, mrc.header.nx\n",
    "\n",
    "#     mrc.print_header()\n",
    "\n",
    "#     # Ensure the MRC file has a cube shape\n",
    "#     if not (mrc.header.nz == mrc.header.ny == mrc.header.nx):\n",
    "#         raise ValueError(\"MRC file is not cubic.\")\n",
    "#     map_size = [int(mrc.header.nz), int(mrc.header.ny), int(mrc.header.nx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = gemmi.read_structure(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the size of the model is not larger than the map\n",
    "box = structure.calculate_box()\n",
    "box_min = list(box.minimum)[::-1]\n",
    "box_max = list(box.maximum)[::-1]\n",
    "\n",
    "if box_max[2] > map_size[2] or box_max[1] > map_size[2] or box_max[0] > map_size[0]:\n",
    "    raise ValueError(\"The model box size exceeds that of the map.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grid_params(box_min_list, box_max_list, axis_length_list, grid_size):\n",
    "    \"\"\"\n",
    "    Computes the starting coordinate and number of grid samples along three axis for a given box.\n",
    "\n",
    "    Args:\n",
    "        box_min_list (List): Minimum coordinates of the box.\n",
    "        box_max_list (List): Maximum coordinates of the box.\n",
    "        axis_length_list (List): Lengths of the axes.\n",
    "        grid_size (int): Size of the grid.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[int]]: Starting coordinates and numbers of grid samples when sampling stride = 0.\n",
    "    \"\"\"\n",
    "    start_coords = []\n",
    "    n_samples = []\n",
    "    for box_min, box_max, axis_length in zip(box_min_list, box_max_list, axis_length_list):\n",
    "        box_mid = (box_min + box_max) // 2\n",
    "        \n",
    "        # num of samples along one axis\n",
    "        n_samples_axis = (box_max - box_min) // grid_size + 1\n",
    "\n",
    "        if not (32 * n_samples_axis <= box_mid <\n",
    "                axis_length - 32 * n_samples_axis):\n",
    "            n_samples_axis -= 1\n",
    "        start_coord = box_mid - 32 * n_samples_axis\n",
    "        start_coords.append(int(start_coord))\n",
    "        n_samples.append(int(n_samples_axis))\n",
    "    return start_coords, n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108, 60, 56] [4, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "# Compute the grid parameters for splitting the volume\n",
    "start_coords, n_samples = compute_grid_params(box_min, box_max, map_size, npy_size)\n",
    "print(start_coords, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_npy(data,\n",
    "                 sample_dir,\n",
    "                 start_coords,\n",
    "                 n_samples,\n",
    "                 npy_size,\n",
    "                 sample_num,\n",
    "                 part_names,\n",
    "                 extract_stride=32):\n",
    "    \"\"\"\n",
    "    Extracts sub-volumes of size npy_size from the input data array, starting from the given start coordinates and generates npy files for each sub-volume.\n",
    "\n",
    "    Parameters:\n",
    "        data (list of ndarray): Input data arrays [tag_data1, tag_data2, ..., map_data]\n",
    "        sample_dir (str): Directory to save the npy files\n",
    "        start_coords (tuple): Tuple of start coordinates (z,y,x)\n",
    "        n_samples (tuple): Tuple of number of samples to extract (z,y,x)\n",
    "        npy_size (int): Size of the sub-volume to extract\n",
    "        extract_stride (int): Stride length of the stepping sample\n",
    "        sample_num (int): Number to use as suffix in the npy file names\n",
    "        part_names (str): Name of the subdirectory to save the npy files\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    num_tags = [0, 0, 0]\n",
    "    sample_start_z, sample_start_y, sample_start_x = start_coords\n",
    "    for i in range(3):\n",
    "        n_samples[i] = int(n_samples[i] * npy_size / extract_stride) - 1\n",
    "    for part_name in part_names:\n",
    "        os.makedirs(os.path.join(sample_dir, part_name), exist_ok=True)\n",
    "    for n_z in range(n_samples[0]):\n",
    "        idx_z = sample_start_z + extract_stride * n_z\n",
    "        for n_y in range(n_samples[1]):\n",
    "            idx_y = sample_start_y + extract_stride * n_y\n",
    "            for n_x in range(n_samples[2]):\n",
    "                idx_x = sample_start_x + extract_stride * n_x\n",
    "                samples = []\n",
    "                counts = []\n",
    "                ratio_tags = []\n",
    "                for idx in range(0, len(data)-1):\n",
    "                    sample = data[idx][idx_z:idx_z + npy_size, idx_y:idx_y + npy_size,\n",
    "                                idx_x:idx_x + npy_size]\n",
    "                    if np.size(sample) == 0:\n",
    "                        continue\n",
    "                    samples.append(sample)\n",
    "                    count = np.bincount(sample.flatten())\n",
    "                    counts.append(count)\n",
    "                    ratio_tag = 1 - count[0] / count.sum()\n",
    "                    ratio_tags.append(ratio_tag)\n",
    "                # print(ratio_tags)\n",
    "                if ratio_tags == [] or max(ratio_tags) < 0.01:\n",
    "                    continue\n",
    "                # Save seperated files for model tags\n",
    "                for idx in range(0, len(data)-1):\n",
    "                    file_name = os.path.join(sample_dir, part_names[idx],\n",
    "                        f\"model_{part_names[idx]}.{sample_num}.npy\")\n",
    "                    np.save(file_name, sample)\n",
    "                    count = counts[idx]\n",
    "                    count = np.pad(count, (0, max(0, 27 - len(count))),\n",
    "                                   'constant')\n",
    "                    num_tags[idx] += count\n",
    "\n",
    "                # Save seperated files for map data\n",
    "                sample = data[len(data)-1][idx_z:idx_z + npy_size,\n",
    "                                    idx_y:idx_y + npy_size,\n",
    "                                    idx_x:idx_x + npy_size]\n",
    "                file_name = os.path.join(sample_dir, part_names[len(data) - 1],\n",
    "                                         f\"map.{sample_num}.npy\")\n",
    "                np.save(file_name, sample)\n",
    "\n",
    "                sample_num += 1\n",
    "\n",
    "    return num_tags, sample_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 0 \n",
    "# # Create npy files from map_data\n",
    "# split_to_npy(map_data, sample_dir, start_coords, n_samples, npy_size,\n",
    "#                 sample_num, 'map_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the selected parts of the model to create the numpy array, record the number of each tag in each part\n",
    "num_of_tag_in_each_part = {}  # Save number of each tag in each part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_coord_cif(structure, RESIDUE=None, ATOM=None):\n",
    "    \"\"\"\n",
    "    Returns the atomic coordinates from a PDB structure for specific residues and atoms.\n",
    "\n",
    "    Args:\n",
    "        structure (Structure): PDB structure.\n",
    "        RESIDUE (list, optional): List of residue names to select. Defaults to None (all residues).\n",
    "        ATOM (list, optional): List of atom names to select. Defaults to None (all atoms).\n",
    "\n",
    "    Returns:\n",
    "        list: List of atomic coordinates as lists [z, y, x].\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if RESIDUE is not None and residue.name not in RESIDUE:\n",
    "                    continue\n",
    "                for atom in residue:\n",
    "                    if ATOM is not None and atom.name not in ATOM:\n",
    "                        continue\n",
    "                    # coords.append(\n",
    "                    #     (int(round(atom.pos.z)), int(round(atom.pos.y)),\n",
    "                    #      int(round(atom.pos.x)))\n",
    "                    # )  \n",
    "                    # coords.append(atom.pos), \n",
    "                    coords.append([atom.pos.z, atom.pos.y, atom.pos.x])\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_npy(model_data,\n",
    "            part_coords,\n",
    "            tag_id,\n",
    "            dis_array=np.array([]),\n",
    "            atom_grid_radius=1.5):\n",
    "    \"\"\"\n",
    "    Add tags to 3D grid points surrounding given part coordinates.\n",
    "\n",
    "    Args:\n",
    "        model_data (ndarray): A 3D numpy array representing the 3D grid.\n",
    "        part_coords (list of tuples): A list of tuples representing the part coordinates (z, y, x).\n",
    "        tag_id (int): The tag value to apply to the tagged points.\n",
    "        atom_grid_radius (int, optional): The radius of the grid to tag around the part coordinates. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        model_data (ndarray): The modified model data with the tagged points.\n",
    "        dis_array (ndarray): The modified model data with the tagged points.\n",
    "    \"\"\"\n",
    "\n",
    "    dis_array = np.full(model_data.shape, atom_grid_radius**2)\n",
    "\n",
    "    index_grid = int(atom_grid_radius) + 1\n",
    "    extension = range(-index_grid, index_grid + 1)\n",
    "    arounds = []\n",
    "    for dz in extension:\n",
    "        for dy in extension:\n",
    "            for dx in extension:\n",
    "                dist = dz**2 + dy**2 + dx**2\n",
    "                if dist <= atom_grid_radius**2:\n",
    "                    arounds.append([dz, dy, dx])\n",
    "    arounds = np.array(arounds)\n",
    "    arounds = np.vstack((arounds, arounds + np.array([0, 0, 1])))\n",
    "    arounds = np.vstack((arounds, arounds + np.array([0, 1, 0])))\n",
    "    arounds = np.vstack((arounds, arounds + np.array([1, 0, 0])))\n",
    "    arounds = np.unique(arounds, axis=0).astype(int)\n",
    "\n",
    "    for coord in np.array(part_coords):\n",
    "        floor_coord = np.floor(coord).astype(int)\n",
    "        for around in arounds:\n",
    "            around_coord = floor_coord + around\n",
    "            dist = np.sum((around_coord - coord) ** 2)\n",
    "            if dist <= dis_array[around_coord[0], around_coord[1], around_coord[2]]:\n",
    "                model_data[around_coord[0], around_coord[1], around_coord[2]] = tag_id\n",
    "                dis_array[around_coord[0], around_coord[1], around_coord[2]] = dist\n",
    "\n",
    "    return model_data, dis_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_2nd_structure_lists(structure):\n",
    "    helices, sheets, = [], []\n",
    "    # helices = [[hlx_chain1, start.res_id.seqid.num, end.res_id.seqid.num],\n",
    "    #            [hlx_chain2, start.res_id.seqid.num, end.res_id.seqid.num],\n",
    "    #            ... ]\n",
    "\n",
    "    for helix_index in range(len(structure.helices)):\n",
    "        start = structure.helices[helix_index].start.res_id.seqid.num\n",
    "        end = structure.helices[helix_index].end.res_id.seqid.num\n",
    "        chain_name = structure.helices[helix_index].start.chain_name\n",
    "        helices.append([chain_name, start, end])\n",
    "\n",
    "    for sheet_index in range(len(structure.sheets)):\n",
    "        for strand_index in range(len(structure.sheets[sheet_index].strands)):\n",
    "            start = structure.sheets[sheet_index].strands[strand_index].start.res_id.seqid.num\n",
    "            end = structure.sheets[sheet_index].strands[strand_index].end.res_id.seqid.num\n",
    "            chain_name = structure.sheets[sheet_index].strands[strand_index].start.chain_name\n",
    "            sheets.append([chain_name, start, end])\n",
    "\n",
    "    return helices, sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_coord_cif_protein_secondary(structure,\n",
    "                                     helices,\n",
    "                                     sheets,\n",
    "                                     RESIDUE=None,\n",
    "                                     ATOM=None):\n",
    "    coords, coords_helices, coords_sheets = [], [], []\n",
    "    chain_helices = [row[0] for row in helices]\n",
    "    chain_sheets = [row[0] for row in sheets]\n",
    "    chain_name = list(set(chain_helices + chain_sheets))\n",
    "\n",
    "    coords = atom_coord_cif(structure, RESIDUE, ATOM)\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            if chain_name is not None and chain.name not in chain_name:\n",
    "                continue\n",
    "            # helices\n",
    "            for index, helix_chian_name in enumerate(chain_helices):\n",
    "                if chain.name != helix_chian_name:\n",
    "                    continue\n",
    "                chain_info = helices[index]\n",
    "                for residue in chain:\n",
    "                    if residue.seqid.num in range(chain_info[1], chain_info[2]+1):\n",
    "                        for atom in residue:\n",
    "                            if ATOM is not None and atom.name not in ATOM:\n",
    "                                continue\n",
    "                            atom_coord = [atom.pos.z, atom.pos.y, atom.pos.x]\n",
    "                            coords_helices.append(atom_coord)\n",
    "            # sheets\n",
    "            for index, sheet_chian_name in enumerate(chain_sheets):\n",
    "                if chain.name != sheet_chian_name:\n",
    "                    continue\n",
    "                chain_info = sheets[index]\n",
    "                for residue in chain:\n",
    "                    if residue.seqid.num in range(chain_info[1], chain_info[2] + 1):\n",
    "                        for atom in residue:\n",
    "                            if ATOM is not None and atom.name not in ATOM:\n",
    "                                continue\n",
    "                            atom_coord = [atom.pos.z, atom.pos.y, atom.pos.x]\n",
    "                            coords_sheets.append(atom_coord)\n",
    "\n",
    "    coords_loops = [x for x in coords if not (x in coords_sheets or x in coords_helices)]\n",
    "    # coords_others1 = [x for x in coords_sheets if x not in coords]\n",
    "    # coords_others2 = [x for x in coords_helices if x not in coords]\n",
    "    # print(coords_others1, coords_others2)\n",
    "\n",
    "    return [coords_helices, coords_sheets, coords_loops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_coords = None\n",
    "model_data = np.zeros(map_size, np.int8)\n",
    "dis_array = np.array([])\n",
    "\n",
    "part_name = \"secondary_strctures\"\n",
    "\n",
    "residues_protein = [\n",
    "    \"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLU\", \"GLN\", \"GLY\", \"HIS\", \"ILE\",\n",
    "    \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\", \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\"\n",
    "]\n",
    "residues_rna = ['A', 'G', 'C', 'U']\n",
    "atoms_protein_backbone = [ 'CA', 'C', 'N']\n",
    "atoms_rna_backbone = [\n",
    "    \"P\", \"OP1\", \"OP2\", \"O5'\", \"C5'\", \"C4'\", \"O4'\", \"C3'\", \"O3'\", \"C2'\", \"O2'\",\n",
    "    \"C1'\"\n",
    "]\n",
    "\n",
    "helices, sheets = protein_2nd_structure_lists(structure)\n",
    "\n",
    "# Tag key atoms(CA+C+N) in helices with 1, in sheets with 2, in loops with 3\n",
    "# Tag key atoms(P+sugar) in rna with 4\n",
    "\n",
    "\n",
    "# for helices, sheets and loops\n",
    "protein_2nd_structure_coords = atom_coord_cif_protein_secondary(\n",
    "    structure, helices, sheets, residues_protein, atoms_protein_backbone)\n",
    "\n",
    "for tag in range(1, len(protein_2nd_structure_coords)+1):\n",
    "    part_coords = protein_2nd_structure_coords[tag - 1]\n",
    "    model_data, dis_array = tag_npy(model_data, part_coords, tag, dis_array)\n",
    "\n",
    "# for rna\n",
    "part_coords = atom_coord_cif(structure, residues_rna, atoms_rna_backbone)\n",
    "model_data, dis_array = tag_npy(model_data, part_coords, 4, dis_array)\n",
    "\n",
    "data.append(model_data)\n",
    "part_names.append(part_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_coords = None\n",
    "model_data = np.zeros(map_size, np.int8)\n",
    "num_of_tag = []\n",
    "dis_array = np.array([])\n",
    "\n",
    "part_name = \"key_atoms\"\n",
    "\n",
    "residues_protein = [\n",
    "    \"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLU\", \"GLN\", \"GLY\", \"HIS\", \"ILE\",\n",
    "    \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\", \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\"\n",
    "]\n",
    "residues_rna = ['A', 'G', 'C', 'U']\n",
    "\n",
    "atoms_sugar_ring_new = [\"C4'\", \"O4'\", \"C3'\", \"C2'\", \"C1'\"]\n",
    "# key_atoms = [[\"CA\"], [\"C\"], [\"N\"], ['P'], atoms_sugar_ring_new]\n",
    "key_atoms = [[\"CA\"], ['P'], atoms_sugar_ring_new]\n",
    "# Tag CA, P, sugar atoms with 1, 2, 3 repectively\n",
    "\n",
    "for tag in range(1, len(key_atoms) + 1):\n",
    "    if key_atoms[tag - 1] in [[\"CA\"]]:\n",
    "        part_coords = atom_coord_cif(structure, residues_protein,\n",
    "                                     key_atoms[tag - 1])\n",
    "    else:\n",
    "        part_coords = atom_coord_cif(structure, residues_rna,\n",
    "                                     key_atoms[tag - 1])\n",
    "    model_data, dis_array = tag_npy(model_data, part_coords, tag, dis_array)\n",
    "\n",
    "data.append(model_data)\n",
    "part_names.append(part_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = np.zeros(map_size, np.int8)\n",
    "dis_array = np.array([])\n",
    "\n",
    "part_name = \"residue_types\"\n",
    "residues_protein = [\n",
    "    \"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLU\", \"GLN\", \"GLY\", \"HIS\", \"ILE\",\n",
    "    \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\", \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\"\n",
    "]\n",
    "residues_rna = ['A', 'G', 'C', 'U']\n",
    "atoms_sugar_ring_new = [\"C4'\", \"O4'\", \"C3'\", \"C2'\", \"C1'\"]\n",
    "\n",
    "\n",
    "# Tag CA atoms with 1, 2, 3, ..., 20 for 20 types of amino acids repectively\n",
    "for tag in range(1, len(residues_protein) + 1):\n",
    "    part_coords = atom_coord_cif(structure, [residues_protein[tag-1]], ['CA'])\n",
    "    model_data, dis_array = tag_npy(model_data, part_coords, tag, dis_array)\n",
    "for base_tag in range(1, len(residues_rna) + 1):\n",
    "    tag = base_tag + len(residues_protein)\n",
    "    part_coords = atom_coord_cif(structure, [residues_rna[base_tag - 1]],\n",
    "                                 atoms_sugar_ring_new)\n",
    "    model_data, dis_array = tag_npy(model_data, part_coords, tag, dis_array)\n",
    "\n",
    "data.append(model_data)\n",
    "part_names.append(part_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append(map_data)\n",
    "part_names.append('map_sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secondary_strctures', 'map_sample']\n",
      "int8\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(part_names)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print(data[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create npy files from model_data of each part and map_data\n",
    "# num_tags, sample_num = split_to_npy(data, sample_dir, start_coords, n_samples,\n",
    "#                                     npy_size, sample_num, part_names)\n",
    "\n",
    "# print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Writing new map\n",
      "New map is writen.\n",
      "=> Writing new map\n",
      "New map is writen.\n"
     ]
    }
   ],
   "source": [
    "GENERATET_MRC_TEST = True\n",
    "CLASSES = 4\n",
    "if GENERATET_MRC_TEST is True:\n",
    "    for idx in range(len(data)):\n",
    "        part_name = part_names[i]\n",
    "        model_data = data[i]\n",
    "        # Generate TEST.mrc for one part\n",
    "        out_map = f\"{map_path.split('.mrc')[0]}_EXAMPLE_{part_name}.mrc\"\n",
    "        print(\"=> Writing new map\")\n",
    "        shutil.copy(map_path, out_map)\n",
    "        with mrcfile.open(out_map, mode='r+') as mrc:\n",
    "            TEST_data = model_data\n",
    "            mrc.set_data(TEST_data)\n",
    "            # mrc.header.mz = model_data.shape[0]\n",
    "        print(\"New map is writen.\")\n",
    "\n",
    "# if GENERATET_MRC_TEST is True:\n",
    "#     for idx in range(len(data)):\n",
    "#         if idx == 1:\n",
    "#             continue\n",
    "#         part_name = part_names[idx]\n",
    "#         model_data = data[idx]\n",
    "#         # Generate TEST.mrc for every part\n",
    "#         for tag in range(1, CLASSES + 1):\n",
    "#             out_map = f\"{map_path.split('.mrc')[0]}_EXAMPLE_{part_name}_{tag}.mrc\"\n",
    "#             print(\"=> Writing new map\")\n",
    "#             shutil.copy(map_path, out_map)\n",
    "#             with mrcfile.open(out_map, mode='r+') as mrc:\n",
    "#                 TEST_data = np.zeros_like(model_data)\n",
    "#                 TEST_data = np.where(model_data == tag, model_data, 0)\n",
    "#                 mrc.set_data(TEST_data)\n",
    "\n",
    "#             print(\"New map is writen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0.1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
