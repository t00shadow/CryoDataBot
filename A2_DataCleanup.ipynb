{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e8d5ff",
   "metadata": {},
   "source": [
    "# Python Script Cleaner for CSV_files CapStone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96ba537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7b2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory that Contains CSV, Subsequent files will be generated into here\n",
    "csv_directory = (r\"C:\\Users\\micha\\OneDrive\\Desktop\\BE177\\FULL_DATA_CLEANUP2\")\n",
    "\n",
    "#MAIN Data File you wish to clean\n",
    "file_name = (r\"OGDATA.csv\")\n",
    "csv_df = pd.read_csv(os.path.join(\"r\",csv_directory,file_name))\n",
    "\n",
    "#First lets drop Entries (Rows) that have NaN or NA values\n",
    "nan_filter = csv_df.dropna()\n",
    "nan_filter = nan_filter.sort_values(by='title') #Want to alphabatize them\n",
    "nan_filter['resolution'] = pd.to_numeric(nan_filter['resolution'])\n",
    "\n",
    "#Lets Save this Starting File with all the OG DATA\n",
    "nan_filter.to_csv(os.path.join(\"r\",csv_directory,\"OGDATA.csv\"), index = False)\n",
    "\n",
    "#Lets save the length of this dataframe, so we know hm was removed from this OG\n",
    "OG_Length = len(nan_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba08a08",
   "metadata": {},
   "source": [
    "## Soft Pass Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7e17b",
   "metadata": {},
   "source": [
    "One of the columns xref_UNIPROTKB keeps track of the ways each Cryo-EM has been referenced. If two entries have the same exact references they will be the same exact protein therefore remove this repeated entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a612ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Removed during Soft Pass:  -52 %\n"
     ]
    }
   ],
   "source": [
    "##Need to filter out similar proteins\n",
    "#Return Type is a series (array) which contains True and False for rows you want to keep or naw depending on last argument\n",
    "non_unique_mask = nan_filter.duplicated(subset='xref_UNIPROTKB', keep=False)\n",
    "non_unique_df = nan_filter[non_unique_mask] #Gives you a dataframe that has all the duplicates\n",
    "unique_df = nan_filter.drop(non_unique_df.index) #Drops the rows that are not unique based on previous dataframe\n",
    "\n",
    "df = unique_df\n",
    "HP_Length = len(unique_df)\n",
    "df.to_csv(os.path.join(csv_directory))\n",
    "\n",
    "file_name = file_name+\"_HARDPASS.csv\"\n",
    "df.to_csv(os.path.join(csv_directory,file_name), index= False) #Lets Save this harpass filtered stuff\n",
    "non_unique_df\n",
    "print(\"% Removed during Soft Pass: \",round(((HP_Length-OG_Length)/OG_Length)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d2ad0",
   "metadata": {},
   "source": [
    "#### Items caught in the softpass dataframe, will now be cleaned to ensure for entries of the same protein only the entry with the lowest resoloution is kept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b7b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Removed From SoftPass:  -19.46 %\n"
     ]
    }
   ],
   "source": [
    "non_unique_df\n",
    "lst = []\n",
    "\n",
    "#Sort by groups of xref_UNIPROTKB\n",
    "grouped_proteins = non_unique_df.groupby(\"xref_UNIPROTKB\")\n",
    "for ref, group in grouped_proteins:\n",
    "    group = group.dropna()\n",
    "    high_res = group.loc[group[\"resolution\"].idxmin()]\n",
    "    lst.append(high_res) #Only append highest-res entries   \n",
    "    \n",
    "#Keep the highest res shit\n",
    "high_res_df = pd.DataFrame(lst,columns = non_unique_df.columns)\n",
    "kept_highRes = len(non_unique_df) - len(high_res_df)\n",
    "\n",
    "#Save it\n",
    "high_res_df.to_csv(os.path.join(\"r\",csv_directory,\"Post_HARDPASS_FILTERED_HIGHRES.csv\"))\n",
    "#print(\"% Removed From SoftPass: \",round((((HP_Length+kept_highRes)-OG_Length)/OG_Length)*100,2),\"%\") ##For Testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece3151",
   "metadata": {},
   "source": [
    "#### Now go back and readd the filtered entries to the dataframe with unique entries, thus all entries contain unique proteins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19135aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Join 2 dataframes\n",
    "result = pd.concat([unique_df, high_res_df], ignore_index=True)\n",
    "result = result.sort_values(by='title')\n",
    "result.to_csv(os.path.join(\"r\",csv_directory,\"RAW_DATA.csv\"))\n",
    "df=result\n",
    "#OG_SP = len(result) ##For testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ade44",
   "metadata": {},
   "source": [
    "## HardPass Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399be6aa",
   "metadata": {},
   "source": [
    "While proteins with the exact same references are filtered out, there exists proteins that have overlapping references. In this case the proteins are not unique from each other thus we need to filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0635a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if there's at least one common sequence\n",
    "def has_common_sequence(row1, row2):\n",
    "    sequences1 = set(row1[\"xref_UNIPROTKB\"].split(','))  # Split and convert to a set for efficient comparison\n",
    "    sequences2 = set(row2[\"xref_UNIPROTKB\"].split(','))\n",
    "    return bool(sequences1 & sequences2)  # Check for common elements using set intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3038473",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_group = {'class':[0]*len(df)}\n",
    "#df_flag = pd.DataFrame(dict_flag)\n",
    "df_group = pd.DataFrame(dict_group)\n",
    "df = pd.concat([df,df_group], ignore_index=False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85954e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 8, 9, 17, 23, 24, 25, 26, 27, 30, 31, 32, 35, 36, 37, 38, 39, 63, 75, 76, 77, 78, 79, 89, 90, 91, 92, 93, 94, 95, 96, 118, 119, 120, 121, 133, 134, 135, 203, 204, 213, 215, 216, 217, 218, 259, 260, 279, 280, 281, 322, 323, 324, 325, 359, 360, 361, 362, 365, 369, 370, 371, 372, 374, 375, 376, 377, 378, 379, 380, 381, 382, 386, 391, 392, 393, 394, 395, 397, 400, 401, 408, 413, 490, 493, 499, 521, 524, 526, 547, 550, 559, 565, 566, 569, 570, 604, 605, 616, 617, 618, 619, 670, 673, 677, 680, 681, 684, 685, 690, 691, 693, 694, 695, 700, 714, 717, 728, 730, 743, 744, 745, 746, 747, 752, 754, 755, 756, 758, 788, 789, 808, 837, 838, 851, 852, 878, 893, 899, 900, 903, 904, 0]\n",
      "[36, 37, 730, 1]\n",
      "[16, 398, 399, 654, 655, 656, 738, 2]\n",
      "[10, 19, 67, 68, 69, 241, 242, 301, 302, 312, 313, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 491, 492, 658, 659, 660, 705, 706, 707, 709, 723, 727, 772, 773, 781, 782, 3]\n",
      "[40, 83, 100, 130, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 202, 209, 210, 263, 264, 265, 266, 267, 268, 269, 270, 271, 318, 330, 335, 336, 339, 358, 363, 402, 479, 480, 481, 482, 522, 546, 548, 640, 657, 668, 669, 682, 698, 699, 713, 733, 785, 790, 791, 792, 815, 822, 841, 842, 843, 855, 857, 858, 905, 5]\n",
      "[567, 6]\n",
      "[12, 20, 21, 22, 845, 846, 847, 11]\n",
      "[710, 713, 13]\n",
      "[58, 74, 80, 81, 172, 173, 527, 528, 607, 608, 762, 763, 15]\n",
      "[113, 383, 384, 500, 534, 567, 568, 18]\n",
      "[34, 132, 488, 651, 652, 653, 737, 738, 739, 740, 741, 849, 28]\n",
      "[802, 803, 33]\n",
      "[43, 208, 255, 507, 508, 509, 510, 515, 516, 606, 609, 610, 611, 612, 613, 614, 626, 696, 697, 731, 732, 734, 775, 776, 780, 805, 806, 810, 41]\n",
      "[136, 137, 432, 515, 536, 42]\n",
      "[577, 871, 873, 898, 44]\n",
      "[814, 46]\n",
      "[48, 49, 50, 51, 52, 53, 54, 272, 273, 47]\n",
      "[56, 55]\n",
      "[154, 164, 314, 591, 593, 594, 57]\n",
      "[62, 60]\n",
      "[517, 518, 519, 520, 64]\n",
      "[234, 235, 236, 237, 65]\n",
      "[71, 72, 531, 70]\n",
      "[347, 73]\n",
      "[115, 126, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 225, 229, 306, 307, 308, 356, 387, 412, 416, 428, 429, 432, 505, 506, 609, 611, 615, 624, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 701, 702, 703, 726, 766, 807, 881, 894, 895, 896, 897, 82]\n",
      "[85, 86, 84]\n",
      "[226, 88]\n",
      "[100, 130, 161, 170, 202, 222, 243, 247, 248, 249, 250, 318, 327, 336, 339, 340, 341, 342, 343, 344, 345, 358, 363, 402, 479, 480, 481, 482, 538, 541, 542, 543, 544, 545, 546, 548, 620, 621, 640, 661, 662, 663, 664, 665, 666, 667, 668, 669, 729, 733, 753, 785, 790, 791, 792, 801, 815, 841, 842, 843, 855, 856, 857, 858, 869, 870, 901, 902, 905, 97]\n",
      "[289, 406, 525, 578, 579, 580, 581, 582, 584, 585, 586, 587, 588, 589, 590, 595, 596, 597, 598, 599, 600, 602, 793, 794, 795, 796, 797, 860, 861, 862, 863, 864, 865, 866, 867, 868, 98]\n",
      "[354, 99]\n",
      "[102, 125, 127, 129, 495, 501, 502, 503, 504, 643, 686, 101]\n",
      "[104, 105, 106, 103]\n",
      "[247, 343, 345, 538, 620, 621, 729, 753, 107]\n",
      "[152, 153, 187, 188, 189, 220, 305, 315, 418, 421, 475, 476, 477, 478, 552, 638, 678, 692, 704, 759, 771, 800, 809, 812, 813, 816, 817, 818, 819, 821, 844, 879, 108]\n",
      "[277, 293, 294, 316, 330, 331, 332, 334, 335, 405, 553, 622, 623, 641, 712, 822, 859, 109]\n",
      "[111, 112, 110]\n",
      "[116, 117, 683, 114]\n",
      "[123, 122]\n",
      "[128, 736, 742, 764, 124]\n",
      "[217, 281, 323, 324, 325, 377, 401, 693, 694, 695, 852, 904, 131]\n",
      "[274, 275, 151]\n",
      "[240, 254, 409, 155]\n",
      "[160, 850, 156]\n",
      "[159, 171, 186, 212, 219, 221, 223, 244, 246, 276, 282, 286, 287, 288, 295, 296, 297, 298, 299, 310, 317, 352, 357, 364, 407, 427, 474, 498, 551, 751, 760, 767, 768, 778, 823, 872, 158]\n",
      "[348, 532, 533, 848, 162]\n",
      "[239, 283, 285, 299, 311, 364, 557, 558, 676, 767, 768, 163]\n",
      "[167, 417, 554, 555, 166]\n",
      "[175, 176, 177, 178, 179, 180, 181, 835, 836, 174]\n",
      "[183, 184, 185, 414, 415, 483, 484, 485, 486, 487, 182]\n",
      "[205, 206, 207, 225, 229, 303, 304, 388, 412, 416, 432, 435, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 540, 609, 611, 615, 632, 633, 634, 635, 726, 805, 806, 807, 810, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 894, 895, 896, 897, 196]\n",
      "[511, 633, 708, 201]\n",
      "[260, 373, 214]\n",
      "[228, 227]\n",
      "[529, 811, 251]\n",
      "[284, 252]\n",
      "[261, 257]\n",
      "[328, 835, 836, 262]\n",
      "[321, 319]\n",
      "[321, 320]\n",
      "[875, 333]\n",
      "[338, 798, 337]\n",
      "[350, 349]\n",
      "[367, 750, 876, 366]\n",
      "[505, 628, 766, 877, 389]\n",
      "[481, 482, 396]\n",
      "[604, 404]\n",
      "[411, 410]\n",
      "[425, 450, 424]\n",
      "[434, 433]\n",
      "[765, 853, 854, 489]\n",
      "[514, 530, 513]\n",
      "[574, 537]\n",
      "[560, 561, 562, 539]\n",
      "[554, 563, 564, 584, 585, 602, 639, 866, 867, 549]\n",
      "[777, 556]\n",
      "[575, 576, 571]\n",
      "[573, 575, 576, 572]\n",
      "[591, 592, 593, 594, 601, 583]\n",
      "[715, 722, 603]\n",
      "[710, 733, 835, 836, 842, 645]\n",
      "[647, 648, 649, 646]\n",
      "[672, 671]\n",
      "[711, 674]\n",
      "[814, 689]\n",
      "[816, 818, 819, 716]\n",
      "[719, 718]\n",
      "[769, 770, 720]\n",
      "[779, 721]\n",
      "[749, 748]\n",
      "[787, 786]\n",
      "[825, 826, 827, 828, 829, 830, 831, 832, 833, 824]\n",
      "\n",
      "91\n",
      "77\n",
      "906\n"
     ]
    }
   ],
   "source": [
    "grouped_dfs = {}\n",
    "indexes_dropped = []\n",
    "xRef = []\n",
    "unique = []\n",
    "for i in range(len(df)):\n",
    "    if i not in indexes_dropped:\n",
    "        xRef = []\n",
    "        key = f'DataFrame_{i + 1}' # Create a key for each DataFrame\n",
    "        hasSeenI = False\n",
    "        for j in range(i+1,len(df)):\n",
    "            if has_common_sequence(df.iloc[i], df.iloc[j]):\n",
    "                hasSeenI = True\n",
    "                xRef.append(j) #Type NP array\n",
    "                indexes_dropped.append(j)\n",
    "                \n",
    "        if (hasSeenI):\n",
    "            xRef.append(i)\n",
    "            #print(xRef)\n",
    "            indexes_dropped.append(i)\n",
    "            #xRefDf_temp = pd.DataFrame(xRefGrp,columns = df.columns)\n",
    "            #xRefDf = xRefDf.concat([xRefDf,xRefDf_temp], axis = 1)\n",
    "            grouped_dfs[key] = xRef\n",
    "        else:\n",
    "            unique.append(i) #If it was a unique row\n",
    "\n",
    "# print()\n",
    "# print(len(grouped_dfs))\n",
    "# print(len(unique))\n",
    "# print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e5098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go Through each group in dataframe and assign same class\n",
    "for i, key in enumerate(grouped_dfs):\n",
    "    for j in grouped_dfs[key]:\n",
    "        df.at[j, 'class'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e00a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_indexes = []\n",
    "non_unique = []\n",
    "for i, key in enumerate(grouped_dfs):\n",
    "    kept_indexes.append(random.choice(grouped_dfs[key]))\n",
    "    non_unique = non_unique + grouped_dfs[key]\n",
    "\n",
    "\n",
    "unique = unique + kept_indexes\n",
    "final_df = df.iloc[unique].reset_index(drop=True)\n",
    "similar_proteins = df.iloc[non_unique].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0039db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_proteins = similar_proteins.sort_values(by='class')\n",
    "similar_proteins.to_csv(os.path.join(\"r\",csv_directory,\"similar_proteins.csv\"), index= False) #Lets Save this harpass filtered stuff\n",
    "final_df.to_csv(os.path.join(csv_directory,\"FINAL_filtered.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f143d8f",
   "metadata": {},
   "source": [
    "## RUN this code on the final filtered file to obtain statistics automatically, or dont its annoying to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b214083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = (r\"C:\\Users\\micha\\OneDrive\\Desktop\\BE177\\FULL_DATA_CLEANUP\\Proteins_Caught_In_Filter\\Post_HARDPASS_FILTERED_HIGHRES.csv\")\n",
    "# csv_directory = r\"C:\\Users\\micha\\OneDrive\\Desktop\\BE177\\FULL_DATA_CLEANUP\\Proteins_Caught_In_Filter\"\n",
    "# final_df_groups = pd.read_csv(csv_file)\n",
    "# final_df_groups = final_df_groups.groupby(final_df_groups['Type'])\n",
    "# Protein_Types = []\n",
    "# Protein_Type_Counts = []\n",
    "# for group in final_df_groups:\n",
    "#     Protein_Types.append(group[0])\n",
    "#     Protein_Type_Counts.append(group[1].shape[0])\n",
    "\n",
    "# final_df_groups = {key : value for key, value in zip(Protein_Types, Protein_Type_Counts)}\n",
    "# final_df_groupz = pd.DataFrame.from_dict([final_df_groups])\n",
    "# final_df_groupz.to_csv(os.path.join(csv_directory,\"InitialCounts.csv\"),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
